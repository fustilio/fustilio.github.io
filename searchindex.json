{"categories":[{"title":"hackathon","uri":"/categories/hackathon/"},{"title":"portfolio","uri":"/categories/portfolio/"},{"title":"side-project","uri":"/categories/side-project/"},{"title":"thoughts","uri":"/categories/thoughts/"}],"posts":[{"content":" I was approached by a professor from NUS who wanted to explore the idea of capturing a crime scene in virtual reality. The goal was to try to see if it was possible to take the subjectivity out of crime scene photography. Crime scene photographers have the heavy job of documenting the scene before the other investigators and examiners can touch anything. These photos will then be constantly referenced during the remainder of investigation and procescution, if any.\nPhotos are traditionally two-dimensioned, but with the use of 360Â° photos, more of the scene can be viewed at once, as one continuous image, placing users in the centre of the crime scene, surely an improvement from flat 2D-images. With simple VR gear like Google Cardboard and the like, the viewer can get a semi-immersive experience; investigators can get an overview and a better spatial understanding of the crime scene. However, the experience locks the viewer at the position of the camera; the viewer isn\u0026rsquo;t able to move closer towards areas of interest or change perspectives by translation but can only explore the scene by rotating the device. To allow translation and moving about in the scene, a 3D model will have to be created and this is typically done using laser scanners whi8ch is rather time-consuming.\nWouldn\u0026rsquo;t it be great if the entire crime scene could be captured quickly, possibly visualized in 3D instead of 2D photos which do not fully convey the scale and positions of the evidence. And at the same time, might it be possible to take the subjectivity of what the photographer captures out of the equation? In my mind, I thought it might be possible to recontruct a crime scene from 360Â° images, how hard could it be?\nForensic Science Advocacy Competition I was given the opportunity to visit a simulated crime scene which was created for a competition. The task was help capture the 360Â° still images with a GoPro Fusion and hopefully try to create something usable. I initially tried to extract out still rectangular images from the 360Â° images I captured in an attempt to use photogrammetry software, which takes in 2D images, to create the 3D model.\nPhotogrammetry Software I tried  Autodesk ReCap Photo  Licensed software Free student license; generate models with up to 50 images per model Cloud-based processing  Meshroom by AliceVision  Free, Open source No photo limit Photos processed locally  RealityCapture  Pay-per-input No photo limit Photos processed locally Control points  3DF Zephyr Free  Free for personal use; generate models with up to 50 images per model 360Â° image decomposition feature   Autodesk ReCap is actually really simple to use, just dump your photos in and a couple of minutes later you get back a pretty decent 3D model. The user-interface (for the student license at least) is pretty limiting as there aren\u0026rsquo;t many parameters you can tune. I then proceeded to look for alternatives which could offer a higher tunability, and Meshroom and RealityCapture came up. Both of them process images locally on my computer which meant long waiting times between attempts. I did appreciate the customizability and the lack of a paywall for Meshroom but ultimately it was the control points feature in RealityCapture that was especially useful for the crime scene. I didn\u0026rsquo;t really try Zephyr to do any 3D model construction as I had access to Autodesk ReCap which I\u0026rsquo;d expect to have similar results.\nThe ability to assign control points was a useful feature as there were many similar points in the images that made it difficult for the software to match automatically. An example would be the blood splatter and drip trails were difficult to differentiate, causing significant distortion and bad texturing. Using control points, we can force the software to use the manually aligned points as reference to stich the model together.\nResults Here\u0026rsquo;s what I came up with:\n 3D Model 360Â° Viewer  Learning Points Through this process, I\u0026rsquo;ve learnt a few important things.\n The photos should be high resolution for better feature matching The photos should be ideally taken with the same device or model to minimize the need to calibrate multiple devices Extraction of still images from a 360Â° image causes the extracted rectangular image from any one viewpoint to have a significantly lower resolution than the marketed resolution of 5.2K.  Potential Extensions If time permits, I would like to also look into using machine learning (I love buzz words) to recognize the blood splatter patterns and augment the crime scene with labels. We\u0026rsquo;ll see how it goes.\n","id":0,"section":"posts","summary":"I was approached by a professor from NUS who wanted to explore the idea of capturing a crime scene in virtual reality. The goal was to try to see if it was possible to take the subjectivity out of crime scene photography. Crime scene photographers have the heavy job of documenting the scene before the other investigators and examiners can touch anything. These photos will then be constantly referenced during the remainder of investigation and procescution, if any.","tags":["virtual-reality","photogrammetry"],"title":"Crime Scene Virtualization","uri":"/2020/02/crime-scene-virtualization/","year":"2020"},{"content":" One in six people have some form of visual impairment ranging from mild vision loss to total blindness. The visually impaired constantly face the danger of walking into people or hazardous objects. This project proposes the use of vibrotactile feedback to serve as an obstacle detection system for visually impaired users. We utilize a mixed reality headset with on-board depth sensors to build a digital map of the real world and a suit with an array of actuators to provide feedback as to indicate to the visually impaired the position of obstacles around them. This is demonstrated by a simple prototype built using commercially available devices (Microsoft HoloLens and bHaptics Tactot) and a qualitative user study was conducted to evaluate the viability of the proposed system. Through our user-testing performed on subjects with simulated visual impairments, our results affirm the potential of using mixed reality to detect obstacles in the environment along with only transmitting essential information through the haptic suit due to limited bandwidth.\nHow it works The HoloLens is able to create a spatial mesh of the environment using its onboard depth cameras. Next, an arc-shaped collider mesh expands outwards from the headset and collides with the generated mesh of the nearest obstacle. Depending on the direction and distance of the detected obstacle, the corresponding actuators on the haptic vest will vibrate. For example, if the obstacle is detected on the left, the actuators on the left haptic vest will vibrate. Also, the frequency of the vibrations correspond to the distance of the obstacles. This system serves as an early warning system for the visually impaired user to detect obstacles without the use of a white cane.\n","id":1,"section":"posts","summary":"One in six people have some form of visual impairment ranging from mild vision loss to total blindness. The visually impaired constantly face the danger of walking into people or hazardous objects. This project proposes the use of vibrotactile feedback to serve as an obstacle detection system for visually impaired users. We utilize a mixed reality headset with on-board depth sensors to build a digital map of the real world and a suit with an array of actuators to provide feedback as to indicate to the visually impaired the position of obstacles around them.","tags":["mixed-reality","hololens","so-i-made-this"],"title":"Enhanced Spatial Awareness for the Visually Impaired using Mixed Reality and Vibrotactile Feedback","uri":"/2020/02/mixed-reality-vision/","year":"2020"},{"content":" DoodleCloud was made as part of the Global VR Hackathon 2019 which I participated with Caryn. We were given 48-hours and the theme of the hackathon was \u0026lsquo;Cloud\u0026rsquo;. We developed a VR drawing game, DoodleCloud, where players would try their best to draw pictures in the sky using their finger.\nFrom past experiences, we knew the importance of having a winning idea before anything. We began our ideation process by penning down words related to \u0026lsquo;Cloud\u0026rsquo; and studying the capabilities of the VR headsets (HTC Vive Pro) and the SDKs that were provided. We spent the first evening brainstorming and our final insights came from looking at images of clouds online.\nA cloud is unique in that it is formless and we based our project on the idea that people imagine different things when they look at clouds in the sky.\n-- Another key aspect would be utilizing the technology provided for us. We employed the Hand-tracking feature to use the playerâ€™s hand as a controller to create an experience as close to the real world as possible; pointing to the sky and clouds.\nFor the gameplay, we first prompt the player with a word and using their index finger, players will draw with the sky as a â€˜canvasâ€™ and clouds as â€˜inkâ€™. DoodleCloud will then determine how close the drawing matches the given word by sending their drawings to Googleâ€™s Quickdraw backend.\nThe screencaptures below illustrate the general flow of the game:\nWe pitched the product as being based on the idea that clouds are formless, the medium of interaction utilizes clouds (the drawing) and lastly, we utilised Googleâ€™s QuickDraw dataset, which (arguably) would be accessible through Google Cloud.\nFuture work I\u0026rsquo;m currently in the process of polishing up the project and porting it over to the Oculus Quest headset to showcase it at an upcoming event!\n","id":2,"section":"posts","summary":"DoodleCloud was made as part of the Global VR Hackathon 2019 which I participated with Caryn. We were given 48-hours and the theme of the hackathon was \u0026lsquo;Cloud\u0026rsquo;. We developed a VR drawing game, DoodleCloud, where players would try their best to draw pictures in the sky using their finger.\nFrom past experiences, we knew the importance of having a winning idea before anything. We began our ideation process by penning down words related to \u0026lsquo;Cloud\u0026rsquo; and studying the capabilities of the VR headsets (HTC Vive Pro) and the SDKs that were provided.","tags":["virtual-reality"],"title":"DoodleCloud: Drawing in the Sky","uri":"/2020/02/doodle-cloud/","year":"2020"},{"content":" Using NFC tags embedded in tactile paving and an NFC reader attached to a white cane to give the visually impaired more information. Background Without assistance, visually impaired individuals might often find themselves in dangerous situations due to their lack of spatial awareness or understanding of their environment. For example, road crossings or approaching escalators in the opposite direction. At these danger areas, tactile paving would allow the visually impaired to be alerted but what if the tactile paving would tell them more? The ConnectCane is a walking aid that would allow the visually impaired to \u0026ldquo;listen\u0026rdquo; to their environment by scanning for information on nfc-enabled tactile pavings. \nDesign The ConnectCane consists of five main components, the NFC reader attached to the tip of a white cane, a speaker, a microcontroller, the cane itself and programmable NFC Tags.\nThe NFC reader will read from the detected NFC tags which will contain information about the location of the paving and what dangers might be around. The information will be processed on-board the device and the device will be constantly updated with new information about tags in more locations.\nThe NFC Tags will have information in the following format: LATLONG:POSTAL:TYPE:INFORMATION\n LATLONG - refers to the latitude and longitude information of the tactile paving for navigation\nPOSTAL - refers to postal code of the area to more specifically define the area\nTYPE - refers to the danger level of the area\nTYPE 0: Safe Zone TYPE 1: Limited Access TYPE 2: Danger Area  INFORMATION - additional details\nAn example of a string embedded in the NFC Tag following the format is as follows 001.2655,103.8221:098585:1:TOILET,MALE that would refer to a male toilet in some shopping mall. This string can be also be shortened by removing characters to become @/$0012655,1038221:098585:1:TLT,M\u0026amp; without losing the essential information. @/$ and \u0026amp; demarcates the start and end of the string respectively. @/$0012655,1038221:098585:2:ESC,U\u0026amp; might refer to an escalator going in the opposite direction which could potentially be dangerous for the user.\nImplementation The ConnectCane was made using the Flip\u0026amp;Click board, NFC Click and TextToSpeech Click by MikroE, and programmed using the Arduino IDE. The microcontroller parses the embedded information in the NFC tags into a user-friendly string that can be read aloud or trigger an alert to warn the user to stay away.\nUsage Simply turn on the ConnectCane and run the tip of the cane between the tactile bumps and over the NFC tags and the information will be read-aloud.\nSecurity The ConnectCane will contain private information such as the location of the user and also contain details about the environment for the different tactile pavings. This information needs to be especially secure as if these information were to be tampered with, the visually impaired might be led towards danger instead of away.\nPotential Extensions The ConnectCane could incorporate navigational cues to bring the user to their desired location through audio or tactile feedback. The user\u0026rsquo;s preferences and information could also be used to make the parsed information user-specific or customized.\n","id":3,"section":"posts","summary":"Using NFC tags embedded in tactile paving and an NFC reader attached to a white cane to give the visually impaired more information. Background Without assistance, visually impaired individuals might often find themselves in dangerous situations due to their lack of spatial awareness or understanding of their environment. For example, road crossings or approaching escalators in the opposite direction. At these danger areas, tactile paving would allow the visually impaired to be alerted but what if the tactile paving would tell them more?","tags":["arduino","hardware","so-i-made-this"],"title":"ConnectCane: Navigation Aid for the Visually Impaired","uri":"/2020/02/connect-cane/","year":"2020"},{"content":" In a previous hackathon I participated in, there was a project that prototyped the use of machine learning to \u0026ldquo;evolve\u0026rdquo; the capability of in-game AI agents by using user provided input through a Virtual Reality game. I was pretty blown away by the idea of evolution through repetitive training and exposure to user-provided input. I\u0026rsquo;ve seen a couple of videos where developers created simulations with Unity and thought that it might be cool to give it a try myself.\nFun with ML-Agents Unity has a ton of cool plugins and frameworks to play around with, and I\u0026rsquo;ve been developing casually with Unity for just about a year now so I thought it would be create my very own agent that would learn to beat me at my own game. I\u0026rsquo;m by no means a machine learning expert so I\u0026rsquo;m glad that Unity had provided ML-Agents as an easy way for developers to utilize some serious machine learning algorithms without having to do any of the heavy lifting.\nUnity ML-Agents Toolkit has many great examples you can launch and play around with to get a feel of the basic functionality and code. Looking at all the examples made me want to try writing my own toy example and I decided that two players playing a game of Pong would be pretty cool; that would be my Hello World program with ML-Agents! But first, I needed to make the game playable and subsequently let the agent take control ðŸ˜ˆ.\nIteration 1: Just Pong The first thing to build was a simple clone of the classic Pong game for the agent to play which was simple enough since it\u0026rsquo;s Pong and even simpler since I decided to make the agent play against a wall to begin with which I thought was a pretty reasonable sparring partner.\nThe next thing to determine would be what inputs to give the agent and what are the actionables for the agent. When playing Pong, to deflect the pong (the ball that bounces around), what would be useful would be the position of the paddle and the pong the velocity of the pong. With ML-Agents, it was as simple as calling a method to add those variables :simple_smile: and after some hand-waving-black-box-machine-learning which I don\u0026rsquo;t really understand, voila, it\u0026rsquo;s alive!\nInitially, the paddle randomly moves about, hitting the pong sometimes but we see that the frequency of misses slowly decreases; this is illustrated by the decreasing number of red flashes, each indicating a reset in the simulations due to a miss.\nEventually, we get a not-so-terrible agent controlling a paddle, able to detect and deflect the pong.\nIteration 2: The Next Dimension Extending the 2D Pong to the next dimension was pretty simple since everything was implemented with 3D assets, all that was needed was to extend the side walls and include a roof on the playing field and of course extending the paddle in the new dimension as well.\nAfter some tweaking, a new agent was trained for the new environment the agent worked just as well.\nIteration 3: Multi-Non-Player With an agent that can now play pong in a 3D environment, we simply have to duplicate that agent and rotate it 180 degrees to play against the original agent. However, I had to change the frame of references for the agents (because now they\u0026rsquo;re viewing things the same way but from different perspectives) and below is the final result!\nResources Unity ML-Agents Toolkit Documentation\nGreat talk by Danny Lange \u0026amp; Arthur Juliani on ML-Agents\n","id":4,"section":"posts","summary":"In a previous hackathon I participated in, there was a project that prototyped the use of machine learning to \u0026ldquo;evolve\u0026rdquo; the capability of in-game AI agents by using user provided input through a Virtual Reality game. I was pretty blown away by the idea of evolution through repetitive training and exposure to user-provided input. I\u0026rsquo;ve seen a couple of videos where developers created simulations with Unity and thought that it might be cool to give it a try myself.","tags":["Unity3D","ML-Agents","random","so-i-made-this"],"title":"So I made this 3D pong game you can't even play","uri":"/2020/01/pong-3d-ml/","year":"2020"},{"content":"Finally got down to creating this site, without any customization and the entirety of the site is pretty much identical to the example site provided. I just wanted some place to pen down my ideas and projects before I forget and in true maker spirit I hope to be able to share some of the lessons I\u0026rsquo;ve learnt along the way. I suppose this will be like my very own twitter or facebook where I can share things with people who are interested without disturbing people who don\u0026rsquo;t really care. Hopefully I can keep up the habit.\n","id":5,"section":"posts","summary":"Finally got down to creating this site, without any customization and the entirety of the site is pretty much identical to the example site provided. I just wanted some place to pen down my ideas and projects before I forget and in true maker spirit I hope to be able to share some of the lessons I\u0026rsquo;ve learnt along the way. I suppose this will be like my very own twitter or facebook where I can share things with people who are interested without disturbing people who don\u0026rsquo;t really care.","tags":["writing"],"title":"Initial Commit","uri":"/2020/01/initial-commit/","year":"2020"}],"tags":[{"title":"arduino","uri":"/tags/arduino/"},{"title":"hardware","uri":"/tags/hardware/"},{"title":"hololens","uri":"/tags/hololens/"},{"title":"mixed-reality","uri":"/tags/mixed-reality/"},{"title":"ML-Agents","uri":"/tags/ml-agents/"},{"title":"photogrammetry","uri":"/tags/photogrammetry/"},{"title":"random","uri":"/tags/random/"},{"title":"so-i-made-this","uri":"/tags/so-i-made-this/"},{"title":"Unity3D","uri":"/tags/unity3d/"},{"title":"virtual-reality","uri":"/tags/virtual-reality/"},{"title":"writing","uri":"/tags/writing/"}]}