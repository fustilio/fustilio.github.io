{"categories":[{"title":"hackathon","uri":"/categories/hackathon/"},{"title":"portfolio","uri":"/categories/portfolio/"},{"title":"side-project","uri":"/categories/side-project/"},{"title":"thoughts","uri":"/categories/thoughts/"}],"posts":[{"content":" One in six people have some form of visual impairment ranging from mild vision loss to total blindness. The visually impaired constantly face the danger of walking into people or hazardous objects. This project proposes the use of vibrotactile feedback to serve as an obstacle detection system for visually impaired users. We utilize a mixed reality headset with on-board depth sensors to build a digital map of the real world and a suit with an array of actuators to provide feedback as to indicate to the visually impaired the position of obstacles around them. This is demonstrated by a simple prototype built using commercially available devices (Microsoft HoloLens and bHaptics Tactot) and a qualitative user study was conducted to evaluate the viability of the proposed system. Through our user-testing performed on subjects with simulated visual impairments, our results affirm the potential of using mixed reality to detect obstacles in the environment along with only transmitting essential information through the haptic suit due to limited bandwidth.\nHow it works The HoloLens is able to create a spatial mesh of the environment using its onboard depth cameras. Next, an arc-shaped collider mesh expands outwards from the headset and collides with the generated mesh of the nearest obstacle. Depending on the direction and distance of the detected obstacle, the corresponding actuators on the haptic vest will vibrate. For example, if the obstacle is detected on the left, the actuators on the left haptic vest will vibrate. Also, the frequency of the vibrations correspond to the distance of the obstacles. This system serves as an early warning system for the visually impaired user to detect obstacles without the use of a white cane.\n","id":0,"section":"posts","summary":"One in six people have some form of visual impairment ranging from mild vision loss to total blindness. The visually impaired constantly face the danger of walking into people or hazardous objects. This project proposes the use of vibrotactile feedback to serve as an obstacle detection system for visually impaired users. We utilize a mixed reality headset with on-board depth sensors to build a digital map of the real world and a suit with an array of actuators to provide feedback as to indicate to the visually impaired the position of obstacles around them.","tags":["mixed-reality","hololens","so-i-made-this"],"title":"Enhanced Spatial Awareness for the Visually Impaired using Mixed Reality and Vibrotactile Feedback","uri":"/2020/02/mixed-reality-vision/","year":"2020"},{"content":" DoodleCloud was made as part of the Global VR Hackathon 2019 which I participated with Caryn. We were given 48-hours and the theme of the hackathon was \u0026lsquo;Cloud\u0026rsquo;. We developed a VR drawing game, DoodleCloud, where players would try their best to draw pictures in the sky using their finger.\nFrom past experiences, we knew the importance of having a winning idea before anything. We began our ideation process by penning down words related to \u0026lsquo;Cloud\u0026rsquo; and studying the capabilities of the VR headsets (HTC Vive Pro) and the SDKs that were provided. We spent the first evening brainstorming and our final insights came from looking at images of clouds online.\nA cloud is unique in that it is formless and we based our project on the idea that people imagine different things when they look at clouds in the sky.\n-- Another key aspect would be utilizing the technology provided for us. We employed the Hand-tracking feature to use the player’s hand as a controller to create an experience as close to the real world as possible; pointing to the sky and clouds.\nFor the gameplay, we first prompt the player with a word and using their index finger, players will draw with the sky as a ‘canvas’ and clouds as ‘ink’. DoodleCloud will then determine how close the drawing matches the given word by sending their drawings to Google’s Quickdraw backend.\nThe screencaptures below illustrate the general flow of the game:\nWe pitched the product as being based on the idea that clouds are formless, the medium of interaction utilizes clouds (the drawing) and lastly, we utilised Google’s QuickDraw dataset, which (arguably) would be accessible through Google Cloud.\nFuture work I\u0026rsquo;m currently in the process of polishing up the project and porting it over to the Oculus Quest headset to showcase it at an upcoming event!\n","id":1,"section":"posts","summary":"DoodleCloud was made as part of the Global VR Hackathon 2019 which I participated with Caryn. We were given 48-hours and the theme of the hackathon was \u0026lsquo;Cloud\u0026rsquo;. We developed a VR drawing game, DoodleCloud, where players would try their best to draw pictures in the sky using their finger.\nFrom past experiences, we knew the importance of having a winning idea before anything. We began our ideation process by penning down words related to \u0026lsquo;Cloud\u0026rsquo; and studying the capabilities of the VR headsets (HTC Vive Pro) and the SDKs that were provided.","tags":["virtual-reality"],"title":"DoodleCloud: Drawing in the Sky","uri":"/2020/02/doodle-cloud/","year":"2020"},{"content":" Using NFC tags embedded in tactile paving and an NFC reader attached to a white cane to give the visually impaired more information. Background Without assistance, visually impaired individuals might often find themselves in dangerous situations due to their lack of spatial awareness or understanding of their environment. For example, road crossings or approaching escalators in the opposite direction. At these danger areas, tactile paving would allow the visually impaired to be alerted but what if the tactile paving would tell them more? The ConnectCane is a walking aid that would allow the visually impaired to \u0026ldquo;listen\u0026rdquo; to their environment by scanning for information on nfc-enabled tactile pavings. \nDesign The ConnectCane consists of five main components, the NFC reader attached to the tip of a white cane, a speaker, a microcontroller, the cane itself and programmable NFC Tags.\nThe NFC reader will read from the detected NFC tags which will contain information about the location of the paving and what dangers might be around. The information will be processed on-board the device and the device will be constantly updated with new information about tags in more locations.\nThe NFC Tags will have information in the following format: LATLONG:POSTAL:TYPE:INFORMATION\n LATLONG - refers to the latitude and longitude information of the tactile paving for navigation\nPOSTAL - refers to postal code of the area to more specifically define the area\nTYPE - refers to the danger level of the area\nTYPE 0: Safe Zone TYPE 1: Limited Access TYPE 2: Danger Area  INFORMATION - additional details\nAn example of a string embedded in the NFC Tag following the format is as follows 001.2655,103.8221:098585:1:TOILET,MALE that would refer to a male toilet in some shopping mall. This string can be also be shortened by removing characters to become @/$0012655,1038221:098585:1:TLT,M\u0026amp; without losing the essential information. @/$ and \u0026amp; demarcates the start and end of the string respectively. @/$0012655,1038221:098585:2:ESC,U\u0026amp; might refer to an escalator going in the opposite direction which could potentially be dangerous for the user.\nImplementation The ConnectCane was made using the Flip\u0026amp;Click board, NFC Click and TextToSpeech Click by MikroE, and programmed using the Arduino IDE. The microcontroller parses the embedded information in the NFC tags into a user-friendly string that can be read aloud or trigger an alert to warn the user to stay away.\nUsage Simply turn on the ConnectCane and run the tip of the cane between the tactile bumps and over the NFC tags and the information will be read-aloud.\nSecurity The ConnectCane will contain private information such as the location of the user and also contain details about the environment for the different tactile pavings. This information needs to be especially secure as if these information were to be tampered with, the visually impaired might be led towards danger instead of away.\nPotential Extensions The ConnectCane could incorporate navigational cues to bring the user to their desired location through audio or tactile feedback. The user\u0026rsquo;s preferences and information could also be used to make the parsed information user-specific or customized.\n","id":2,"section":"posts","summary":"Using NFC tags embedded in tactile paving and an NFC reader attached to a white cane to give the visually impaired more information. Background Without assistance, visually impaired individuals might often find themselves in dangerous situations due to their lack of spatial awareness or understanding of their environment. For example, road crossings or approaching escalators in the opposite direction. At these danger areas, tactile paving would allow the visually impaired to be alerted but what if the tactile paving would tell them more?","tags":["arduino","hardware","so-i-made-this"],"title":"ConnectCane: Navigation Aid for the Visually Impaired","uri":"/2020/02/connect-cane/","year":"2020"},{"content":" In a previous hackathon I participated in, there was a project that prototyped the use of machine learning to \u0026ldquo;evolve\u0026rdquo; the capability of in-game AI agents by using user provided input through a Virtual Reality game. I was pretty blown away by the idea of evolution through repetitive training and exposure to user-provided input. I\u0026rsquo;ve seen a couple of videos where developers created simulations with Unity and thought that it might be cool to give it a try myself.\nFun with ML-Agents Unity has a ton of cool plugins and frameworks to play around with, and I\u0026rsquo;ve been developing casually with Unity for just about a year now so I thought it would be create my very own agent that would learn to beat me at my own game. I\u0026rsquo;m by no means a machine learning expert so I\u0026rsquo;m glad that Unity had provided ML-Agents as an easy way for developers to utilize some serious machine learning algorithms without having to do any of the heavy lifting.\nUnity ML-Agents Toolkit has many great examples you can launch and play around with to get a feel of the basic functionality and code. Looking at all the examples made me want to try writing my own toy example and I decided that two players playing a game of Pong would be pretty cool; that would be my Hello World program with ML-Agents! But first, I needed to make the game playable and subsequently let the agent take control 😈.\nIteration 1: Just Pong The first thing to build was a simple clone of the classic Pong game for the agent to play which was simple enough since it\u0026rsquo;s Pong and even simpler since I decided to make the agent play against a wall to begin with which I thought was a pretty reasonable sparring partner.\nThe next thing to determine would be what inputs to give the agent and what are the actionables for the agent. When playing Pong, to deflect the pong (the ball that bounces around), what would be useful would be the position of the paddle and the pong the velocity of the pong. With ML-Agents, it was as simple as calling a method to add those variables :simple_smile: and after some hand-waving-black-box-machine-learning which I don\u0026rsquo;t really understand, voila, it\u0026rsquo;s alive!\nInitially, the paddle randomly moves about, hitting the pong sometimes but we see that the frequency of misses slowly decreases; this is illustrated by the decreasing number of red flashes, each indicating a reset in the simulations due to a miss.\nEventually, we get a not-so-terrible agent controlling a paddle, able to detect and deflect the pong.\nIteration 2: The Next Dimension Extending the 2D Pong to the next dimension was pretty simple since everything was implemented with 3D assets, all that was needed was to extend the side walls and include a roof on the playing field and of course extending the paddle in the new dimension as well.\nAfter some tweaking, a new agent was trained for the new environment the agent worked just as well.\nIteration 3: Multi-Non-Player With an agent that can now play pong in a 3D environment, we simply have to duplicate that agent and rotate it 180 degrees to play against the original agent. However, I had to change the frame of references for the agents (because now they\u0026rsquo;re viewing things the same way but from different perspectives) and below is the final result!\nResources Unity ML-Agents Toolkit Documentation\nGreat talk by Danny Lange \u0026amp; Arthur Juliani on ML-Agents\n","id":3,"section":"posts","summary":"In a previous hackathon I participated in, there was a project that prototyped the use of machine learning to \u0026ldquo;evolve\u0026rdquo; the capability of in-game AI agents by using user provided input through a Virtual Reality game. I was pretty blown away by the idea of evolution through repetitive training and exposure to user-provided input. I\u0026rsquo;ve seen a couple of videos where developers created simulations with Unity and thought that it might be cool to give it a try myself.","tags":["Unity3D","ML-Agents","random","so-i-made-this"],"title":"So I made this 3D pong game you can't even play","uri":"/2020/01/pong-3d-ml/","year":"2020"},{"content":"Finally got down to creating this site, without any customization and the entirety of the site is pretty much identical to the example site provided. I just wanted some place to pen down my ideas and projects before I forget and in true maker spirit I hope to be able to share some of the lessons I\u0026rsquo;ve learnt along the way. I suppose this will be like my very own twitter or facebook where I can share things with people who are interested without disturbing people who don\u0026rsquo;t really care. Hopefully I can keep up the habit.\n","id":4,"section":"posts","summary":"Finally got down to creating this site, without any customization and the entirety of the site is pretty much identical to the example site provided. I just wanted some place to pen down my ideas and projects before I forget and in true maker spirit I hope to be able to share some of the lessons I\u0026rsquo;ve learnt along the way. I suppose this will be like my very own twitter or facebook where I can share things with people who are interested without disturbing people who don\u0026rsquo;t really care.","tags":["writing"],"title":"Initial Commit","uri":"/2020/01/initial-commit/","year":"2020"}],"tags":[{"title":"arduino","uri":"/tags/arduino/"},{"title":"hardware","uri":"/tags/hardware/"},{"title":"hololens","uri":"/tags/hololens/"},{"title":"mixed-reality","uri":"/tags/mixed-reality/"},{"title":"ML-Agents","uri":"/tags/ml-agents/"},{"title":"random","uri":"/tags/random/"},{"title":"so-i-made-this","uri":"/tags/so-i-made-this/"},{"title":"Unity3D","uri":"/tags/unity3d/"},{"title":"virtual-reality","uri":"/tags/virtual-reality/"},{"title":"writing","uri":"/tags/writing/"}]}